{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "09f73c6b-be4c-4265-b97f-ff6e9f44260b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification, BertTokenizerFast\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForTokenClassification, pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import json\n",
    "import evaluate\n",
    "import numpy\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3e37bf12-4bd4-4c7a-83e9-2b1e66536ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11672, 1945)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = ['B-LOC', 'I-LOC', 'O']\n",
    "# 假设这是你的训练数据，以tokenized形式表示\n",
    "with open('data_gen.json', 'r', encoding='utf-8') as f:\n",
    "    datas = json.load(f)\n",
    "  \n",
    "new_datas = []\n",
    "for idx, data in enumerate(datas):\n",
    "    new_dict = {}\n",
    "    new_dict['id'] = idx\n",
    "    new_dict['tokens'] = data['text']\n",
    "    new_dict['ner_tags'] = data['ner_tags']\n",
    "    new_datas.append(new_dict)\n",
    "    \n",
    "# 转换成datasets库的格式\n",
    "from datasets import Dataset\n",
    "shuffle(new_datas)\n",
    "\n",
    "train_dataset = Dataset.from_list(train_datas[len(new_datas) // 7 : ])\n",
    "val_dataset = Dataset.from_list(train_datas[: len(new_datas)  // 7])\n",
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac0d2f5b-7cd1-40ad-9c21-6135dda573f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'address', 'ner_tags'],\n",
       "    num_rows: 1945\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b68b726-f009-4890-ac50-4b169645b634",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 0\n",
      "家 0\n",
      "住 0\n",
      "在 0\n",
      "岳 1\n",
      "阳 2\n",
      "市 2\n",
      "君 2\n",
      "山 2\n",
      "区 2\n",
      "君 2\n",
      "山 2\n",
      "街 2\n",
      "道 2\n",
      "1 2\n",
      "2 2\n",
      "3 2\n",
      "号 2\n",
      "， 0\n",
      "这 0\n",
      "附 0\n",
      "近 0\n",
      "有 0\n",
      "个 0\n",
      "垃 0\n",
      "圾 0\n",
      "桶 0\n",
      "被 0\n",
      "堵 0\n",
      "住 0\n",
      "了 0\n",
      "， 0\n",
      "希 0\n",
      "望 0\n",
      "能 0\n",
      "尽 0\n",
      "快 0\n",
      "清 0\n",
      "理 0\n",
      "一 0\n",
      "下 0\n",
      "。 0\n"
     ]
    }
   ],
   "source": [
    "a = train_dataset[1]\n",
    "for i, j in zip(a['text'], a['ner_tags']):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "273c16ff-abcc-48d4-ac8d-2017a15ca0fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# 加载BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('google-bert/bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43665979-2924-4577-bb76-bef12dc386e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a845297e-ba29-4ac5-97a2-86dd4e41f7bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52ddcfbd16a41fbb5bdbd8a5f5fd0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11672 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972a10dcf69b4ca3852cfedd95dd872a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1945 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 借助word_ids 实现标签映射\n",
    "def process_function(datas):\n",
    "    tokenized_datas = tokenizer(datas[\"text\"], max_length=256, truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(datas[\"ner_tags\"]):\n",
    "        word_ids = tokenized_datas.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_id])\n",
    "        labels.append(label_ids)\n",
    "    tokenized_datas[\"labels\"] = labels\n",
    "    return tokenized_datas\n",
    "\n",
    "\n",
    "new_train_datasets = train_dataset.map(process_function, batched=True)\n",
    "new_val_datasets = val_dataset.map(process_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0dc2f80b-5498-489a-9280-243ca79a962b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[CLS] 我 家 在 中 山 市 南 区 某 小 区 ， 我 家 旁 边 的 垃 圾 桶 老 是 被 放 错 地 方 [SEP]',\n",
       " [-100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(new_train_datasets[15]['input_ids']), new_train_datasets[15]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd6247eb-7f6e-4eb8-9e4e-c511a7a09f52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 加载BERT模型，用于NER任务微调\n",
    "model = BertForTokenClassification.from_pretrained('google-bert/bert-base-chinese',\n",
    "                                                   num_labels=3)  ## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b49af99-4ea6-4a23-a696-c2a52ad214cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc014d6f-58d9-4e81-9018-97b9957c60f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc3580e4-518b-4032-bca5-83e4d758fa90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# step 4 评估函数：此处的评估函数可以从https://github.com/huggingface/evaluate下载到本地\n",
    "seqeval = evaluate.load(\"./evaluate_/seqeval_metric.py\")\n",
    "\n",
    "\n",
    "def evaluate_function(prepredictions):\n",
    "    predictions, labels = prepredictions\n",
    "    predictions = numpy.argmax(predictions, axis=-1)\n",
    "    # 将id转换为原始的字符串类型的标签\n",
    "    true_predictions = [\n",
    "        [label_list[p] for p, l in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    true_labels = [\n",
    "        [label_list[l] for p, l in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    result = seqeval.compute(predictions=true_predictions, references=true_labels, mode=\"strict\", scheme=\"IOB2\")\n",
    "    return {\n",
    "        \"f1\": result[\"overall_f1\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b99abeca-567c-458d-ae18-232e11880bde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# 训练设置\n",
    "train_args = TrainingArguments(output_dir=\"./checkpoints\",      # 输出文件夹\n",
    "                               per_device_train_batch_size=32,  # 训练时的batch_size\n",
    "                               # gradient_checkpointing=True,     # *** 梯度检查点 ***\n",
    "                               per_device_eval_batch_size=32,    # 验证时的batch_size\n",
    "                               num_train_epochs=3,              # 训练轮数\n",
    "                               logging_steps=20,                # log 打印的频率\n",
    "                               evaluation_strategy=\"epoch\",     # 评估策略\n",
    "                               save_strategy=\"epoch\",           # 保存策略\n",
    "                               save_total_limit=3,              # 最大保存数\n",
    "                               metric_for_best_model=\"f1\",      # 设定评估指标\n",
    "                               load_best_model_at_end=True      # 训练完成后加载最优模型\n",
    "                               )\n",
    "\n",
    "# 训练器\n",
    "# step 6 创建Trainer\n",
    "trainer = Trainer(model=model,\n",
    "                  args=train_args,\n",
    "                  train_dataset=new_train_datasets,\n",
    "                  eval_dataset=new_val_datasets,\n",
    "                  data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer),\n",
    "                  compute_metrics=evaluate_function,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "90024309-25b3-4224-b825-3c4709412038",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1095' max='1095' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1095/1095 01:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>0.023726</td>\n",
       "      <td>0.994593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.024385</td>\n",
       "      <td>0.994698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.023677</td>\n",
       "      <td>0.995086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1095, training_loss=0.013654830874917714, metrics={'train_runtime': 70.9483, 'train_samples_per_second': 493.542, 'train_steps_per_second': 15.434, 'total_flos': 1084232115130896.0, 'train_loss': 0.013654830874917714, 'epoch': 3.0})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开始训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1920604d-ae50-4cca-ac1a-5fc53c0a4370",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.023677131161093712, 'eval_f1': 0.995086381414729, 'eval_runtime': 3.3167, 'eval_samples_per_second': 586.42, 'eval_steps_per_second': 18.392, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# step 8 模型评估\n",
    "evaluate_result = trainer.evaluate(new_val_datasets)\n",
    "print(evaluate_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4165c089-c15c-4e2d-ae10-43e948aa946e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'LABEL_0', 'score': 0.99999464, 'word': '我 家 住 在', 'start': 0, 'end': 4}, {'entity_group': 'LABEL_1', 'score': 0.758535, 'word': '江', 'start': 4, 'end': 5}, {'entity_group': 'LABEL_2', 'score': 0.9126012, 'word': '苏 省 宿 迁 市 宿 城 区', 'start': 5, 'end': 13}, {'entity_group': 'LABEL_0', 'score': 0.99999344, 'word': '， 我 有 问 题 需 要 反 应 ，', 'start': 13, 'end': 23}, {'entity_group': 'LABEL_2', 'score': 0.94298434, 'word': '北 京 国 贸 大 厦', 'start': 23, 'end': 29}, {'entity_group': 'LABEL_0', 'score': 0.99998933, 'word': '有 个 地 方 洼 陷 ， 希 望 能 求 助', 'start': 29, 'end': 41}]\n"
     ]
    }
   ],
   "source": [
    "# step 9：模型预测\n",
    "ner_pipe = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, device=0, aggregation_strategy=\"simple\")\n",
    "res = ner_pipe(\"我家住在江苏省宿迁市宿城区，我有问题需要反应，北京国贸大厦有个地方洼陷，希望能求助\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd776f5d-195c-4192-a21a-703fa2df232b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fab96bc5-3e77-4496-85f1-f2c0f851d118",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NER_model/tokenizer_config.json',\n",
       " 'NER_model/special_tokens_map.json',\n",
       " 'NER_model/vocab.txt',\n",
       " 'NER_model/added_tokens.json',\n",
       " 'NER_model/tokenizer.json')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('NER_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd0a9b-e980-4fb2-8060-02990222bc9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
