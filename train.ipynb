{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "09f73c6b-be4c-4265-b97f-ff6e9f44260b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification, BertTokenizerFast\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForTokenClassification, pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import json\n",
    "import evaluate\n",
    "import numpy\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3e37bf12-4bd4-4c7a-83e9-2b1e66536ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11672, 1945)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = ['B-LOC', 'I-LOC', 'O']\n",
    "# å‡è®¾è¿™æ˜¯ä½ çš„è®­ç»ƒæ•°æ®ï¼Œä»¥tokenizedå½¢å¼è¡¨ç¤º\n",
    "with open('data_gen.json', 'r', encoding='utf-8') as f:\n",
    "    datas = json.load(f)\n",
    "  \n",
    "new_datas = []\n",
    "for idx, data in enumerate(datas):\n",
    "    new_dict = {}\n",
    "    new_dict['id'] = idx\n",
    "    new_dict['tokens'] = data['text']\n",
    "    new_dict['ner_tags'] = data['ner_tags']\n",
    "    new_datas.append(new_dict)\n",
    "    \n",
    "# è½¬æ¢æˆdatasetsåº“çš„æ ¼å¼\n",
    "from datasets import Dataset\n",
    "shuffle(new_datas)\n",
    "\n",
    "train_dataset = Dataset.from_list(train_datas[len(new_datas) // 7 : ])\n",
    "val_dataset = Dataset.from_list(train_datas[: len(new_datas)  // 7])\n",
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac0d2f5b-7cd1-40ad-9c21-6135dda573f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'address', 'ner_tags'],\n",
       "    num_rows: 1945\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b68b726-f009-4890-ac50-4b169645b634",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘ 0\n",
      "å®¶ 0\n",
      "ä½ 0\n",
      "åœ¨ 0\n",
      "å²³ 1\n",
      "é˜³ 2\n",
      "å¸‚ 2\n",
      "å› 2\n",
      "å±± 2\n",
      "åŒº 2\n",
      "å› 2\n",
      "å±± 2\n",
      "è¡— 2\n",
      "é“ 2\n",
      "1 2\n",
      "2 2\n",
      "3 2\n",
      "å· 2\n",
      "ï¼Œ 0\n",
      "è¿™ 0\n",
      "é™„ 0\n",
      "è¿‘ 0\n",
      "æœ‰ 0\n",
      "ä¸ª 0\n",
      "åƒ 0\n",
      "åœ¾ 0\n",
      "æ¡¶ 0\n",
      "è¢« 0\n",
      "å µ 0\n",
      "ä½ 0\n",
      "äº† 0\n",
      "ï¼Œ 0\n",
      "å¸Œ 0\n",
      "æœ› 0\n",
      "èƒ½ 0\n",
      "å°½ 0\n",
      "å¿« 0\n",
      "æ¸… 0\n",
      "ç† 0\n",
      "ä¸€ 0\n",
      "ä¸‹ 0\n",
      "ã€‚ 0\n"
     ]
    }
   ],
   "source": [
    "a = train_dataset[1]\n",
    "for i, j in zip(a['text'], a['ner_tags']):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "273c16ff-abcc-48d4-ac8d-2017a15ca0fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# åŠ è½½BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('google-bert/bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43665979-2924-4577-bb76-bef12dc386e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a845297e-ba29-4ac5-97a2-86dd4e41f7bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52ddcfbd16a41fbb5bdbd8a5f5fd0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11672 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972a10dcf69b4ca3852cfedd95dd872a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1945 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# å€ŸåŠ©word_ids å®ç°æ ‡ç­¾æ˜ å°„\n",
    "def process_function(datas):\n",
    "    tokenized_datas = tokenizer(datas[\"text\"], max_length=256, truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(datas[\"ner_tags\"]):\n",
    "        word_ids = tokenized_datas.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_id])\n",
    "        labels.append(label_ids)\n",
    "    tokenized_datas[\"labels\"] = labels\n",
    "    return tokenized_datas\n",
    "\n",
    "\n",
    "new_train_datasets = train_dataset.map(process_function, batched=True)\n",
    "new_val_datasets = val_dataset.map(process_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0dc2f80b-5498-489a-9280-243ca79a962b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[CLS] æˆ‘ å®¶ åœ¨ ä¸­ å±± å¸‚ å— åŒº æŸ å° åŒº ï¼Œ æˆ‘ å®¶ æ— è¾¹ çš„ åƒ åœ¾ æ¡¶ è€ æ˜¯ è¢« æ”¾ é”™ åœ° æ–¹ [SEP]',\n",
       " [-100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(new_train_datasets[15]['input_ids']), new_train_datasets[15]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd6247eb-7f6e-4eb8-9e4e-c511a7a09f52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# åŠ è½½BERTæ¨¡å‹ï¼Œç”¨äºNERä»»åŠ¡å¾®è°ƒ\n",
    "model = BertForTokenClassification.from_pretrained('google-bert/bert-base-chinese',\n",
    "                                                   num_labels=3)  ## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b49af99-4ea6-4a23-a696-c2a52ad214cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc014d6f-58d9-4e81-9018-97b9957c60f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc3580e4-518b-4032-bca5-83e4d758fa90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# step 4 è¯„ä¼°å‡½æ•°ï¼šæ­¤å¤„çš„è¯„ä¼°å‡½æ•°å¯ä»¥ä»https://github.com/huggingface/evaluateä¸‹è½½åˆ°æœ¬åœ°\n",
    "seqeval = evaluate.load(\"./evaluate_/seqeval_metric.py\")\n",
    "\n",
    "\n",
    "def evaluate_function(prepredictions):\n",
    "    predictions, labels = prepredictions\n",
    "    predictions = numpy.argmax(predictions, axis=-1)\n",
    "    # å°†idè½¬æ¢ä¸ºåŸå§‹çš„å­—ç¬¦ä¸²ç±»å‹çš„æ ‡ç­¾\n",
    "    true_predictions = [\n",
    "        [label_list[p] for p, l in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    true_labels = [\n",
    "        [label_list[l] for p, l in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    result = seqeval.compute(predictions=true_predictions, references=true_labels, mode=\"strict\", scheme=\"IOB2\")\n",
    "    return {\n",
    "        \"f1\": result[\"overall_f1\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b99abeca-567c-458d-ae18-232e11880bde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# è®­ç»ƒè®¾ç½®\n",
    "train_args = TrainingArguments(output_dir=\"./checkpoints\",      # è¾“å‡ºæ–‡ä»¶å¤¹\n",
    "                               per_device_train_batch_size=32,  # è®­ç»ƒæ—¶çš„batch_size\n",
    "                               # gradient_checkpointing=True,     # *** æ¢¯åº¦æ£€æŸ¥ç‚¹ ***\n",
    "                               per_device_eval_batch_size=32,    # éªŒè¯æ—¶çš„batch_size\n",
    "                               num_train_epochs=3,              # è®­ç»ƒè½®æ•°\n",
    "                               logging_steps=20,                # log æ‰“å°çš„é¢‘ç‡\n",
    "                               evaluation_strategy=\"epoch\",     # è¯„ä¼°ç­–ç•¥\n",
    "                               save_strategy=\"epoch\",           # ä¿å­˜ç­–ç•¥\n",
    "                               save_total_limit=3,              # æœ€å¤§ä¿å­˜æ•°\n",
    "                               metric_for_best_model=\"f1\",      # è®¾å®šè¯„ä¼°æŒ‡æ ‡\n",
    "                               load_best_model_at_end=True      # è®­ç»ƒå®ŒæˆååŠ è½½æœ€ä¼˜æ¨¡å‹\n",
    "                               )\n",
    "\n",
    "# è®­ç»ƒå™¨\n",
    "# step 6 åˆ›å»ºTrainer\n",
    "trainer = Trainer(model=model,\n",
    "                  args=train_args,\n",
    "                  train_dataset=new_train_datasets,\n",
    "                  eval_dataset=new_val_datasets,\n",
    "                  data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer),\n",
    "                  compute_metrics=evaluate_function,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "90024309-25b3-4224-b825-3c4709412038",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1095' max='1095' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1095/1095 01:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>0.023726</td>\n",
       "      <td>0.994593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.024385</td>\n",
       "      <td>0.994698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.023677</td>\n",
       "      <td>0.995086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1095, training_loss=0.013654830874917714, metrics={'train_runtime': 70.9483, 'train_samples_per_second': 493.542, 'train_steps_per_second': 15.434, 'total_flos': 1084232115130896.0, 'train_loss': 0.013654830874917714, 'epoch': 3.0})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å¼€å§‹è®­ç»ƒ\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1920604d-ae50-4cca-ac1a-5fc53c0a4370",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.023677131161093712, 'eval_f1': 0.995086381414729, 'eval_runtime': 3.3167, 'eval_samples_per_second': 586.42, 'eval_steps_per_second': 18.392, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# step 8 æ¨¡å‹è¯„ä¼°\n",
    "evaluate_result = trainer.evaluate(new_val_datasets)\n",
    "print(evaluate_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4165c089-c15c-4e2d-ae10-43e948aa946e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'LABEL_0', 'score': 0.99999464, 'word': 'æˆ‘ å®¶ ä½ åœ¨', 'start': 0, 'end': 4}, {'entity_group': 'LABEL_1', 'score': 0.758535, 'word': 'æ±Ÿ', 'start': 4, 'end': 5}, {'entity_group': 'LABEL_2', 'score': 0.9126012, 'word': 'è‹ çœ å®¿ è¿ å¸‚ å®¿ åŸ åŒº', 'start': 5, 'end': 13}, {'entity_group': 'LABEL_0', 'score': 0.99999344, 'word': 'ï¼Œ æˆ‘ æœ‰ é—® é¢˜ éœ€ è¦ å åº” ï¼Œ', 'start': 13, 'end': 23}, {'entity_group': 'LABEL_2', 'score': 0.94298434, 'word': 'åŒ— äº¬ å›½ è´¸ å¤§ å¦', 'start': 23, 'end': 29}, {'entity_group': 'LABEL_0', 'score': 0.99998933, 'word': 'æœ‰ ä¸ª åœ° æ–¹ æ´¼ é™· ï¼Œ å¸Œ æœ› èƒ½ æ±‚ åŠ©', 'start': 29, 'end': 41}]\n"
     ]
    }
   ],
   "source": [
    "# step 9ï¼šæ¨¡å‹é¢„æµ‹\n",
    "ner_pipe = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, device=0, aggregation_strategy=\"simple\")\n",
    "res = ner_pipe(\"æˆ‘å®¶ä½åœ¨æ±Ÿè‹çœå®¿è¿å¸‚å®¿åŸåŒºï¼Œæˆ‘æœ‰é—®é¢˜éœ€è¦ååº”ï¼ŒåŒ—äº¬å›½è´¸å¤§å¦æœ‰ä¸ªåœ°æ–¹æ´¼é™·ï¼Œå¸Œæœ›èƒ½æ±‚åŠ©\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd776f5d-195c-4192-a21a-703fa2df232b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fab96bc5-3e77-4496-85f1-f2c0f851d118",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NER_model/tokenizer_config.json',\n",
       " 'NER_model/special_tokens_map.json',\n",
       " 'NER_model/vocab.txt',\n",
       " 'NER_model/added_tokens.json',\n",
       " 'NER_model/tokenizer.json')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('NER_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd0a9b-e980-4fb2-8060-02990222bc9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
